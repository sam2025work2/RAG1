{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "import uuid\n",
        "import os\n",
        "from typing import List, Dict, Optional, Any\n",
        "import ollama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load PDF documents\n",
        "loader = PyPDFLoader(\"../data/example.pdf\")\n",
        "docs = loader.load()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DocumentChunker class\n",
        "class DocumentChunker:\n",
        "    \"\"\"A class to chunk documents into smaller pieces for RAG applications.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        chunk_size: int = 1000,\n",
        "        chunk_overlap: int = 200,\n",
        "        separators: List[str] = None,\n",
        "        method: str = \"recursive\"\n",
        "    ):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.method = method\n",
        "        \n",
        "        if method == \"recursive\":\n",
        "            self.splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=chunk_size,\n",
        "                chunk_overlap=chunk_overlap,\n",
        "                separators=separators,\n",
        "                length_function=len\n",
        "            )\n",
        "        else:\n",
        "            self.splitter = CharacterTextSplitter(\n",
        "                chunk_size=chunk_size,\n",
        "                chunk_overlap=chunk_overlap,\n",
        "                separator=\"\\n\\n\",\n",
        "                length_function=len\n",
        "            )\n",
        "    \n",
        "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split a list of documents into chunks.\"\"\"\n",
        "        all_chunks = []\n",
        "        \n",
        "        for doc in documents:\n",
        "            chunks = self.splitter.split_text(doc.page_content)\n",
        "            \n",
        "            for i, chunk_text in enumerate(chunks):\n",
        "                chunk_metadata = doc.metadata.copy()\n",
        "                chunk_metadata.update({\n",
        "                    \"chunk_index\": i,\n",
        "                    \"total_chunks\": len(chunks),\n",
        "                    \"chunk_size\": len(chunk_text)\n",
        "                })\n",
        "                \n",
        "                chunk_doc = Document(\n",
        "                    page_content=chunk_text,\n",
        "                    metadata=chunk_metadata\n",
        "                )\n",
        "                all_chunks.append(chunk_doc)\n",
        "        \n",
        "        return all_chunks\n",
        "    \n",
        "    def chunk_single_document(self, document: Document) -> List[Document]:\n",
        "        \"\"\"Split a single document into chunks.\"\"\"\n",
        "        return self.chunk_documents([document])\n",
        "\n",
        "# Initialize chunker and chunk documents\n",
        "chunker = DocumentChunker(chunk_size=1000, chunk_overlap=200)\n",
        "chunked_docs = chunker.chunk_documents(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EmbeddingManager class\n",
        "class EmbeddingManager:\n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-V2\"):\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self._load_model()\n",
        "    \n",
        "    def _load_model(self):\n",
        "        try:\n",
        "            self.model = SentenceTransformer(self.model_name)\n",
        "        except Exception as ex:\n",
        "            raise ValueError(f\"Error loading model {self.model_name}: {ex}\")\n",
        "    \n",
        "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
        "        if not self.model:\n",
        "            raise ValueError(\"Model not loaded\")\n",
        "        \n",
        "        embeddings = self.model.encode(texts, show_progress_bar=False)\n",
        "        return embeddings\n",
        "\n",
        "# Initialize EmbeddingManager and generate embeddings\n",
        "embedding_manager = EmbeddingManager()\n",
        "texts = [doc.page_content for doc in chunked_docs]\n",
        "embeddings = embedding_manager.generate_embeddings(texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VectorStoreManager class\n",
        "class VectorStoreManager:\n",
        "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
        "        self.collection_name = collection_name\n",
        "        self.persist_directory = persist_directory\n",
        "        self.client = None\n",
        "        self.collection = None\n",
        "        self._initialize_store()\n",
        "    \n",
        "    def _initialize_store(self):\n",
        "        try:\n",
        "            os.makedirs(self.persist_directory, exist_ok=True)\n",
        "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
        "            self.collection = self.client.get_or_create_collection(\n",
        "                name=self.collection_name,\n",
        "                metadata={\"description\": \"PDF documents for RAG\"}\n",
        "            )\n",
        "        except Exception as ex:\n",
        "            raise ValueError(f\"Error initializing store: {ex}\")\n",
        "    \n",
        "    def add_documents(self, documents: List[Document], embeddings: np.ndarray):\n",
        "        if len(documents) != len(embeddings):\n",
        "            raise ValueError(\"Number of documents and embeddings must match\")\n",
        "        \n",
        "        ids = []\n",
        "        metadatas = []\n",
        "        documents_texts = []\n",
        "        embeddings_list = []\n",
        "        \n",
        "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
        "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
        "            ids.append(doc_id)\n",
        "            metadata = dict(doc.metadata)\n",
        "            metadata['doc_index'] = i\n",
        "            metadata['content_length'] = len(doc.page_content)\n",
        "            metadatas.append(metadata)\n",
        "            documents_texts.append(doc.page_content)\n",
        "            embeddings_list.append(embedding.tolist())\n",
        "        \n",
        "        try:\n",
        "            self.collection.add(\n",
        "                ids=ids,\n",
        "                documents=documents_texts,\n",
        "                metadatas=metadatas,\n",
        "                embeddings=embeddings_list\n",
        "            )\n",
        "        except Exception as ex:\n",
        "            raise ValueError(f\"Error adding documents to collection: {ex}\")\n",
        "\n",
        "# Initialize VectorStoreManager and add documents\n",
        "vector_store_manager = VectorStoreManager()\n",
        "vector_store_manager.add_documents(chunked_docs, embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RagRetriever class\n",
        "class RagRetriever:\n",
        "    \"\"\"A RAG (Retrieval-Augmented Generation) retriever for querying vector stores.\"\"\"\n",
        "    \n",
        "    def __init__(self, vector_store_manager, embedding_manager):\n",
        "        self.vector_store_manager = vector_store_manager\n",
        "        self.embedding_manager = embedding_manager\n",
        "        self.collection = vector_store_manager.collection\n",
        "    \n",
        "    def query(self, query_text: str, n_results: int = 15, score_threshold: float = 0.0):\n",
        "        \"\"\"Query the vector store with a text query.\"\"\"\n",
        "        try:\n",
        "            query_embedding = self.embedding_manager.generate_embeddings([query_text])\n",
        "            \n",
        "            results = self.collection.query(\n",
        "                query_embeddings=[query_embedding[0].tolist()],\n",
        "                n_results=n_results\n",
        "            )\n",
        "            \n",
        "            retrieved_docs = []\n",
        "            \n",
        "            if results.get('documents') and len(results['documents']) > 0:\n",
        "                documents = results['documents'][0]\n",
        "                metadatas = results.get('metadatas', [[]])[0] if results.get('metadatas') else []\n",
        "                distances = results.get('distances', [[]])[0] if results.get('distances') else []\n",
        "                ids = results.get('ids', [[]])[0] if results.get('ids') else []\n",
        "                \n",
        "                if len(documents) > 0:\n",
        "                    for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
        "                        similarity_score = 1 - distance\n",
        "                        if similarity_score >= score_threshold:\n",
        "                            retrieved_docs.append({\n",
        "                                'id': doc_id,\n",
        "                                'content': document,\n",
        "                                'metadata': metadata,\n",
        "                                'similarity_score': similarity_score,\n",
        "                                'distance': distance,\n",
        "                                'rank': i+1\n",
        "                            })\n",
        "            \n",
        "            return retrieved_docs\n",
        "        except Exception as ex:\n",
        "            raise ValueError(f\"Error querying collection: {ex}\")\n",
        "\n",
        "# Initialize RAG Retriever\n",
        "rag_retriever = RagRetriever(vector_store_manager, embedding_manager)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Improved RAGChain class with better prompt engineering and context management\n",
        "class RAGChain:\n",
        "    \"\"\"A RAG (Retrieval-Augmented Generation) chain that combines document retrieval with LLM-powered response generation using Ollama.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        rag_retriever,\n",
        "        model_name: str = \"llama2\",\n",
        "        num_context_docs: int = 5,\n",
        "        temperature: float = 0.7,\n",
        "        max_tokens: Optional[int] = None,\n",
        "        min_similarity_score: float = 0.3,  # Filter out low-relevance documents\n",
        "        rerank_results: bool = True  # Rerank by similarity score\n",
        "    ):\n",
        "        self.rag_retriever = rag_retriever\n",
        "        self.model_name = model_name\n",
        "        self.num_context_docs = num_context_docs\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "        self.min_similarity_score = min_similarity_score\n",
        "        self.rerank_results = rerank_results\n",
        "        self._verify_ollama()\n",
        "    \n",
        "    def _verify_ollama(self):\n",
        "        \"\"\"Verify that Ollama is running and the model is available.\"\"\"\n",
        "        try:\n",
        "            models = ollama.list()\n",
        "            available_models = [model['name'] for model in models.get('models', [])]\n",
        "            \n",
        "            model_found = False\n",
        "            for model in available_models:\n",
        "                if self.model_name in model or model in self.model_name:\n",
        "                    model_found = True\n",
        "                    self.model_name = model\n",
        "                    break\n",
        "        except Exception:\n",
        "            pass\n",
        "    \n",
        "    def _filter_and_rerank_docs(self, context_docs: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Filter documents by similarity score and optionally rerank them.\"\"\"\n",
        "        # Filter by minimum similarity score\n",
        "        filtered_docs = [\n",
        "            doc for doc in context_docs \n",
        "            if doc.get('similarity_score', 0) >= self.min_similarity_score\n",
        "        ]\n",
        "        \n",
        "        # Rerank by similarity score (highest first)\n",
        "        if self.rerank_results:\n",
        "            filtered_docs = sorted(\n",
        "                filtered_docs, \n",
        "                key=lambda x: x.get('similarity_score', 0), \n",
        "                reverse=True\n",
        "            )\n",
        "        \n",
        "        return filtered_docs\n",
        "    \n",
        "    def _format_prompt(self, query: str, context_docs: List[Dict]) -> str:\n",
        "        \"\"\"Format the prompt with retrieved context and user query using improved prompt engineering.\"\"\"\n",
        "        if not context_docs:\n",
        "            prompt = f\"\"\"You are a helpful and knowledgeable assistant. Answer the following question to the best of your ability.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Provide a clear, comprehensive answer:\"\"\"\n",
        "        else:\n",
        "            # Format context with better structure and metadata\n",
        "            context_parts = []\n",
        "            for i, doc in enumerate(context_docs, 1):\n",
        "                content = doc.get('content', '').strip()\n",
        "                metadata = doc.get('metadata', {})\n",
        "                source = metadata.get('source', 'Unknown')\n",
        "                page = metadata.get('page', 'N/A')\n",
        "                similarity = doc.get('similarity_score', 0)\n",
        "                \n",
        "                # Only include relevant context\n",
        "                if content:\n",
        "                    context_parts.append(\n",
        "                        f\"--- Document {i} (Relevance: {similarity:.2f}, Source: {source}, Page: {page}) ---\\n{content}\"\n",
        "                    )\n",
        "            \n",
        "            context_text = \"\\n\\n\".join(context_parts)\n",
        "            \n",
        "            # Improved prompt with better instructions\n",
        "            prompt = f\"\"\"You are an expert assistant that answers questions based on provided context documents. Follow these guidelines:\n",
        "\n",
        "1. Answer the question using ONLY the information from the context documents below\n",
        "2. If the answer is not in the context, clearly state \"I cannot find this information in the provided documents\"\n",
        "3. Synthesize information from multiple documents when relevant\n",
        "4. Be specific and cite which document(s) you used (e.g., \"According to Document 1...\")\n",
        "5. If the context is insufficient, explain what information is missing\n",
        "6. Provide a clear, well-structured answer\n",
        "\n",
        "Context Documents:\n",
        "{context_text}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        \n",
        "        return prompt\n",
        "    \n",
        "    def generate(\n",
        "        self,\n",
        "        query: str,\n",
        "        num_context_docs: Optional[int] = None,\n",
        "        temperature: Optional[float] = None,\n",
        "        max_tokens: Optional[int] = None,\n",
        "        min_similarity_score: Optional[float] = None\n",
        "    ) -> Dict:\n",
        "        \"\"\"Generate a response using RAG (Retrieval-Augmented Generation).\"\"\"\n",
        "        num_docs = num_context_docs if num_context_docs is not None else self.num_context_docs\n",
        "        temp = temperature if temperature is not None else self.temperature\n",
        "        max_toks = max_tokens if max_tokens is not None else self.max_tokens\n",
        "        min_score = min_similarity_score if min_similarity_score is not None else self.min_similarity_score\n",
        "        \n",
        "        # Retrieve more documents than needed, then filter and rerank\n",
        "        retrieve_count = max(num_docs * 2, 15)  # Retrieve more for better filtering\n",
        "        context_docs = self.rag_retriever.query(query, n_results=retrieve_count)\n",
        "        \n",
        "        # Temporarily set min_similarity_score for filtering\n",
        "        original_min_score = self.min_similarity_score\n",
        "        self.min_similarity_score = min_score\n",
        "        filtered_docs = self._filter_and_rerank_docs(context_docs)\n",
        "        self.min_similarity_score = original_min_score\n",
        "        \n",
        "        # Take top N documents after filtering\n",
        "        filtered_docs = filtered_docs[:num_docs]\n",
        "        \n",
        "        # If no documents pass the filter, use the best ones anyway\n",
        "        if not filtered_docs and context_docs:\n",
        "            filtered_docs = sorted(\n",
        "                context_docs, \n",
        "                key=lambda x: x.get('similarity_score', 0), \n",
        "                reverse=True\n",
        "            )[:num_docs]\n",
        "        \n",
        "        prompt = self._format_prompt(query, filtered_docs)\n",
        "        \n",
        "        options = {'temperature': temp}\n",
        "        if max_toks is not None:\n",
        "            options['num_predict'] = max_toks\n",
        "        \n",
        "        try:\n",
        "            response = ollama.generate(\n",
        "                model=self.model_name,\n",
        "                prompt=prompt,\n",
        "                options=options\n",
        "            )\n",
        "            \n",
        "            generated_text = response.get('response', '').strip()\n",
        "            \n",
        "            return {\n",
        "                'response': generated_text,\n",
        "                'context_docs': filtered_docs,\n",
        "                'model': self.model_name,\n",
        "                'query': query,\n",
        "                'num_context_docs': len(filtered_docs),\n",
        "                'avg_similarity': sum(d.get('similarity_score', 0) for d in filtered_docs) / len(filtered_docs) if filtered_docs else 0\n",
        "            }\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error generating response: {e}\"\n",
        "            return {\n",
        "                'response': f\"Error: {error_msg}\",\n",
        "                'context_docs': filtered_docs,\n",
        "                'model': self.model_name,\n",
        "                'query': query,\n",
        "                'num_context_docs': len(filtered_docs),\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "# Initialize RAGChain\n",
        "rag_chain = RAGChain(\n",
        "    rag_retriever=rag_retriever,\n",
        "    model_name=\"llama2\",\n",
        "    num_context_docs=5,\n",
        "    temperature=0.7\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Thank you for entrusting me with this task! After carefully reviewing the document, I can confidently confirm that the main topic of this document is... (drumroll please)... Artificial Intelligence! ðŸ¤–\\n\\nYes, the document primarily focuses on the current state and future potential of AI, including its applications, challenges, and implications for various industries and aspects of society. From natural language processing to machine learning, robotics to computer vision, the document provides a comprehensive overview of the rapidly evolving field of AI and its impact on our world. ðŸ’»\\n\\nSo, there you have it! The main topic of this document is indeed Artificial Intelligence. I hope this answers your question accurately and helps you understand the content of the document better. If you have any further questions or need additional clarification, please feel free to ask! ðŸ˜Š'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example usage\n",
        "query = \"What is the main topic of this document?\"\n",
        "result = rag_chain.generate(query)\n",
        "result['response']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "QUERY: What is the main topic of this document?\n",
            "================================================================================\n",
            "\n",
            "RESPONSE:\n",
            "Based on the content provided in the document, the main topic appears to be \"Understanding and Using Artificial Intelligence\". The document provides an overview of artificial intelligence, its applications, and the challenges and limitations associated with it. It also discusses the ethical considerations and potential risks involved in the development and deployment of AI systems. Therefore, the main topic of this document is the understanding and utilization of artificial intelligence.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "METRICS:\n",
            "- Context documents used: 0\n",
            "- Average similarity score: 0.000\n",
            "- Model: llama2\n"
          ]
        }
      ],
      "source": [
        "# Initialize improved RAGChain with optimized settings\n",
        "rag_chain = RAGChain(\n",
        "    rag_retriever=rag_retriever,\n",
        "    model_name=\"llama2\",\n",
        "    num_context_docs=10,\n",
        "    temperature=0.5,  # Lower temperature for more focused, factual answers\n",
        "    min_similarity_score=0.3,  # Filter out low-relevance documents\n",
        "    rerank_results=True  # Rerank by similarity for better context\n",
        ")\n",
        "\n",
        "# Test query\n",
        "query = \"What is the main topic of this document?\"\n",
        "result = rag_chain.generate(query)\n",
        "\n",
        "# Display results\n",
        "print(\"=\" * 80)\n",
        "print(\"QUERY:\", query)\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nRESPONSE:\")\n",
        "print(result['response'])\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"\\nMETRICS:\")\n",
        "print(f\"- Context documents used: {result['num_context_docs']}\")\n",
        "print(f\"- Average similarity score: {result.get('avg_similarity', 0):.3f}\")\n",
        "print(f\"- Model: {result['model']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tips for Improving RAG Answers\n",
        "\n",
        "### 1. **Adjust Similarity Threshold**\n",
        "- Lower `min_similarity_score` (e.g., 0.2) to include more context, but may add noise\n",
        "- Higher `min_similarity_score` (e.g., 0.5) for more precise but potentially incomplete answers\n",
        "- Monitor `avg_similarity` in results to find the sweet spot\n",
        "\n",
        "### 2. **Optimize Context Size**\n",
        "- Start with 5-10 documents, increase if answers are incomplete\n",
        "- Too many documents can confuse the model or hit token limits\n",
        "- Use `num_context_docs` parameter to experiment\n",
        "\n",
        "### 3. **Temperature Settings**\n",
        "- **0.0-0.3**: Very focused, deterministic answers (best for factual queries)\n",
        "- **0.4-0.7**: Balanced creativity and accuracy (default)\n",
        "- **0.8-1.0**: More creative but less reliable\n",
        "\n",
        "### 4. **Better Embedding Models**\n",
        "Consider upgrading the embedding model for better retrieval:\n",
        "- `all-mpnet-base-v2` - Better quality, slower\n",
        "- `multi-qa-mpnet-base-dot-v1` - Optimized for Q&A\n",
        "- `sentence-transformers/all-MiniLM-L6-v2` - Current default, good balance\n",
        "\n",
        "### 5. **Chunking Strategy**\n",
        "- Adjust `chunk_size` (500-1500) based on document type\n",
        "- Increase `chunk_overlap` (200-400) for better context continuity\n",
        "- Consider semantic chunking for better boundaries\n",
        "\n",
        "### 6. **Query Refinement**\n",
        "- Make queries specific and clear\n",
        "- Use keywords that match document terminology\n",
        "- Break complex questions into simpler sub-questions\n",
        "\n",
        "### 7. **Model Selection**\n",
        "- Try newer models: `llama3`, `mistral`, `phi3` for better reasoning\n",
        "- Some models are better at following instructions\n",
        "- Test different models and compare results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "COMPARING DIFFERENT CONFIGURATIONS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Configuration: High Precision (Fewer, High-Quality Docs)\n",
            "  - Context docs: 5\n",
            "  - Min similarity: 0.5\n",
            "  - Temperature: 0.3\n",
            "================================================================================\n",
            "\n",
            "Response: Based on the information provided in the document, the main topic appears to be \"How to Create an Effective Business Plan\". The document provides a detailed outline and guidelines for creating a business plan, including identifying the business's mission and goals, conducting market research, develo...\n",
            "\n",
            "Metrics: 0 docs, avg similarity: 0.000\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Configuration: Balanced (Current Best)\n",
            "  - Context docs: 10\n",
            "  - Min similarity: 0.3\n",
            "  - Temperature: 0.5\n",
            "================================================================================\n",
            "\n",
            "Response: Based on the content provided in the document, the main topic appears to be the use of artificial intelligence (AI) and machine learning (ML) in various industries and their potential impacts on society. The document discusses the benefits and challenges of AI and ML, including their potential to im...\n",
            "\n",
            "Metrics: 0 docs, avg similarity: 0.000\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Configuration: High Recall (More Context)\n",
            "  - Context docs: 15\n",
            "  - Min similarity: 0.2\n",
            "  - Temperature: 0.5\n",
            "================================================================================\n",
            "\n",
            "Response: Based on the information provided in the document, the main topic appears to be \"How to Create an Effective Business Plan\". The document provides a detailed outline and guidelines for creating a business plan, including defining the business, setting goals and objectives, conducting market research,...\n",
            "\n",
            "Metrics: 0 docs, avg similarity: 0.000\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example: Experimenting with different parameters to improve answers\n",
        "\n",
        "query = \"What is the main topic of this document?\"\n",
        "\n",
        "# Try different configurations\n",
        "configs = [\n",
        "    {\n",
        "        \"name\": \"High Precision (Fewer, High-Quality Docs)\",\n",
        "        \"num_context_docs\": 5,\n",
        "        \"min_similarity_score\": 0.5,\n",
        "        \"temperature\": 0.3\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Balanced (Current Best)\",\n",
        "        \"num_context_docs\": 10,\n",
        "        \"min_similarity_score\": 0.3,\n",
        "        \"temperature\": 0.5\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"High Recall (More Context)\",\n",
        "        \"num_context_docs\": 15,\n",
        "        \"min_similarity_score\": 0.2,\n",
        "        \"temperature\": 0.5\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPARING DIFFERENT CONFIGURATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for config in configs:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Configuration: {config['name']}\")\n",
        "    print(f\"  - Context docs: {config['num_context_docs']}\")\n",
        "    print(f\"  - Min similarity: {config['min_similarity_score']}\")\n",
        "    print(f\"  - Temperature: {config['temperature']}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    result = rag_chain.generate(\n",
        "        query,\n",
        "        num_context_docs=config['num_context_docs'],\n",
        "        min_similarity_score=config['min_similarity_score'],\n",
        "        temperature=config['temperature']\n",
        "    )\n",
        "    \n",
        "    print(f\"Response: {result['response'][:300]}...\")  # First 300 chars\n",
        "    print(f\"\\nMetrics: {result['num_context_docs']} docs, avg similarity: {result.get('avg_similarity', 0):.3f}\")\n",
        "    print(\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
