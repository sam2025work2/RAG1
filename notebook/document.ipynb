{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f106524a",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1177d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a604a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n",
      "{'source': 'example.pdf', 'author': 'John Doe', 'date': '2021-01-01'}\n"
     ]
    }
   ],
   "source": [
    "doc = Document(page_content=\"Hello, world!\", metadata={\"source\": \"example.pdf\", \"author\": \"John Doe\",\"date\": \"2021-01-01\"})\n",
    "doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18932791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadhanas/sideProject/rag1/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../data/example.pdf\")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c179ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'Skia/PDF m88', 'creator': 'Adobe Acrobat Pro 11.0.9', 'creationdate': '2020-11-09T15:54:35+02:00', 'source': '../data/example.pdf', 'file_path': '../data/example.pdf', 'total_pages': 658, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-11-09T16:02:22+02:00', 'trapped': '', 'modDate': \"D:20201109160222+02'00'\", 'creationDate': \"D:20201109155435+02'00'\", 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "loaderNew = PyMuPDFLoader(\"../data/example.pdf\")\n",
    "docsNew = loaderNew.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de9cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 658\n",
      "This is because PyPDFLoader creates ONE document per PAGE\n",
      "Your PDF has 658 pages\n",
      "\n",
      "First document metadata:\n",
      "{'producer': 'Skia/PDF m88', 'creator': 'Adobe Acrobat Pro 11.0.9', 'creationdate': '2020-11-09T15:54:35+02:00', 'moddate': '2020-11-09T16:02:22+02:00', 'title': '', 'source': '../data/example.pdf', 'total_pages': 658, 'page': 0, 'page_label': '1'}\n",
      "{'producer': 'Skia/PDF m88', 'creator': 'Adobe Acrobat Pro 11.0.9', 'creationdate': '2020-11-09T15:54:35+02:00', 'moddate': '2020-11-09T16:02:22+02:00', 'title': '', 'source': '../data/example.pdf', 'total_pages': 658, 'page': 657, 'page_label': '658'}\n"
     ]
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d113840",
   "metadata": {},
   "source": [
    "## Why Multiple Documents?\n",
    "\n",
    "**PyPDFLoader creates one Document object per page** of the PDF. This is by design and beneficial for RAG applications:\n",
    "\n",
    "1. **Granular Retrieval**: You can retrieve specific pages relevant to a query\n",
    "2. **Manageable Chunk Sizes**: Each page is a separate unit for embedding\n",
    "3. **Better Context**: Page-level metadata helps track where information came from\n",
    "\n",
    "If you need a single combined document, see the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa853c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined document length: 1089938 characters\n",
      "Combined document metadata: {'source': '../data/example.pdf', 'total_pages': 658, 'combined': True}\n"
     ]
    }
   ],
   "source": [
    "# Optional: Combine all pages into a single document\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Combine all page contents\n",
    "combined_content = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Create a single document with combined metadata\n",
    "combined_doc = Document(\n",
    "    page_content=combined_content,\n",
    "    metadata={\n",
    "        \"source\": docs[0].metadata.get(\"source\"),\n",
    "        \"total_pages\": len(docs),\n",
    "        \"combined\": True\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through documents\n",
    "for doc in docs:\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb888c8",
   "metadata": {},
   "source": [
    "## Text Chunking\n",
    "\n",
    "Chunking breaks down large documents into smaller, manageable pieces for better embedding and retrieval. This is especially useful when:\n",
    "- Pages are too long for effective embeddings\n",
    "- You want more granular retrieval\n",
    "- You need to respect token limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf702cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DocumentChunker with default settings...\n",
      "Chunk size: 1000, Overlap: 200\n"
     ]
    }
   ],
   "source": [
    "# Try modern import first, fallback to legacy import\n",
    "try:\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "except ImportError:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "class DocumentChunker:\n",
    "    \"\"\"\n",
    "    A class to chunk documents into smaller pieces for RAG applications.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        separators: List[str] = None,\n",
    "        method: str = \"recursive\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the chunker.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: Maximum size of chunks (in characters)\n",
    "            chunk_overlap: Overlap between chunks (in characters) to preserve context\n",
    "            separators: List of separators to use for splitting (default: None uses smart defaults)\n",
    "            method: \"recursive\" (smart splitting) or \"character\" (simple character-based)\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.method = method\n",
    "        \n",
    "        if method == \"recursive\":\n",
    "            self.splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                separators=separators,\n",
    "                length_function=len\n",
    "            )\n",
    "        else:  # character-based\n",
    "            self.splitter = CharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                separator=\"\\n\\n\",\n",
    "                length_function=len\n",
    "            )\n",
    "    \n",
    "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Split a list of documents into chunks.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of Document objects to chunk\n",
    "            \n",
    "        Returns:\n",
    "            List of Document objects (chunks) with preserved metadata\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            # Split the document content\n",
    "            chunks = self.splitter.split_text(doc.page_content)\n",
    "            \n",
    "            # Create new Document objects for each chunk with metadata\n",
    "            for i, chunk_text in enumerate(chunks):\n",
    "                chunk_metadata = doc.metadata.copy()\n",
    "                chunk_metadata.update({\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                    \"chunk_size\": len(chunk_text)\n",
    "                })\n",
    "                \n",
    "                chunk_doc = Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata=chunk_metadata\n",
    "                )\n",
    "                all_chunks.append(chunk_doc)\n",
    "        \n",
    "        return all_chunks\n",
    "    \n",
    "    def chunk_single_document(self, document: Document) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Split a single document into chunks.\n",
    "        \n",
    "        Args:\n",
    "            document: Document object to chunk\n",
    "            \n",
    "        Returns:\n",
    "            List of Document objects (chunks)\n",
    "        \"\"\"\n",
    "        return self.chunk_documents([document])\n",
    "\n",
    "# Example usage\n",
    "chunker = DocumentChunker(chunk_size=1000, chunk_overlap=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e5409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking all pages of the PDF...\n",
      "This may take a while for 658 pages...\n",
      "\n",
      "Original documents: 658\n",
      "Chunked documents: 1527\n",
      "\n",
      "First chunk preview:\n",
      "Content length: 195 characters\n",
      "Metadata: {'producer': 'Skia/PDF m88', 'creator': 'Adobe Acrobat Pro 11.0.9', 'creationdate': '2020-11-09T15:54:35+02:00', 'moddate': '2020-11-09T16:02:22+02:00', 'title': '', 'source': '../data/example.pdf', 'total_pages': 658, 'page': 0, 'page_label': '1', 'chunk_index': 0, 'total_chunks': 1, 'chunk_size': 195}\n",
      "\n",
      "Content preview (first 200 chars):\n",
      "Preface - Neural Networks from Scratch in Python\n",
      " \n",
      "2\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Neural Networks\n",
      " \n",
      " \n",
      " \n",
      "from Scratch in\n",
      " \n",
      " \n",
      " \n",
      "Python\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Harrison Kinsley & Daniel Kukieła...\n"
     ]
    }
   ],
   "source": [
    "# Example: Chunk all pages of the PDF\n",
    "all_docs = docs  # All pages\n",
    "chunked_docs = chunker.chunk_documents(all_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf6877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Chunk all documents (all pages)\n",
    "# Uncomment to chunk the entire PDF (this may take a while for 658 pages)\n",
    "# print(\"Chunking all pages...\")\n",
    "# all_chunked_docs = chunker.chunk_documents(docs)\n",
    "# print(f\"Original: {len(docs)} pages\")\n",
    "# print(f\"After chunking: {len(all_chunked_docs)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e038ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom chunker created:\n",
      "Chunk size: 2000\n",
      "Overlap: 400\n"
     ]
    }
   ],
   "source": [
    "# Example: Custom chunking with different parameters\n",
    "# For longer context windows or different use cases\n",
    "custom_chunker = DocumentChunker(\n",
    "    chunk_size=2000,      # Larger chunks\n",
    "    chunk_overlap=400,     # More overlap for better context\n",
    "    method=\"recursive\"     # Smart splitting\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7143091c",
   "metadata": {},
   "source": [
    "### Embedding and Vector Store  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77fabfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer \n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9ff8587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model all-MiniLM-L6-V2\n",
      "Model dimensions 384\n",
      " Generating embeddings for 1527 texts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 48/48 [00:05<00:00,  9.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with shape (1527, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name:str = \"all-MiniLM-L6-V2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "        except Exception as ex:\n",
    "            raise ValueError(f\"Error loading model {self.model_name}: {ex}\")\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.model :\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        embeddings = self.model.encode(texts, show_progress_bar=False)\n",
    "        return embeddings\n",
    "    \n",
    "## Intialize Embedding Manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "## Generate Embeddings\n",
    "# Extract page_content from Document objects to get list of strings\n",
    "texts = [doc.page_content for doc in chunked_docs]\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b20d92",
   "metadata": {},
   "source": [
    "### VectoreStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022128ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chroma client for collection pdf_documents...\n",
      "Collection pdf_documents initialized successfully\n",
      "Existing documents in collection: 1527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStoreManager at 0x30f16e630>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents.base import Document\n",
    "import os\n",
    "from typing import Any\n",
    "class VectorStoreManager:\n",
    "    def __init__(self, collection_name:str = \"pdf_documents\", persist_directory:str = \"../data/vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "    \n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF documents for RAG\"}\n",
    "            )\n",
    "        except Exception as ex:\n",
    "            raise ValueError(f\"Error initializing store: {ex}\")\n",
    "    \n",
    "    def add_documents(self, documents: List[Document], embeddings: np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents and embeddings must match\")\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_texts = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = 1\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            documents_texts.append(doc.page_content)\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                documents=documents_texts,\n",
    "                metadatas=metadatas,\n",
    "                embeddings=embeddings_list\n",
    "            )\n",
    "        except Exception as ex:\n",
    "            raise ValueError(f\"Error adding documents to collection: {ex}\")\n",
    "\n",
    "vector_store_manager = VectorStoreManager()\n",
    "vector_store_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagRetriever:\n",
    "    \"\"\"\n",
    "    A RAG (Retrieval-Augmented Generation) retriever for querying vector stores.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store_manager, embedding_manager):\n",
    "        \"\"\"\n",
    "        Initialize the RAG Retriever.\n",
    "        \n",
    "        Args:\n",
    "            vector_store_manager: VectorStoreManager instance with the document collection\n",
    "            embedding_manager: EmbeddingManager instance to generate query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store_manager = vector_store_manager\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.collection = vector_store_manager.collection\n",
    "    \n",
    "    def query(self, query_text: str, n_results: int = 15, score_threshold: float = 0.0):\n",
    "        \"\"\"\n",
    "        Query the vector store with a text query.\n",
    "        \n",
    "        Args:\n",
    "            query_text: The query text to search for\n",
    "            n_results: Number of results to return\n",
    "            score_threshold: Minimum similarity score (0.0 to 1.0) to include results\n",
    "            \n",
    "        Returns:\n",
    "            Query results with documents, metadata, and distances\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate embedding for the query using the same model as documents\n",
    "            query_embedding = self.embedding_manager.generate_embeddings([query_text])\n",
    "            \n",
    "            # Query using the embedding (not query_texts) to match stored embeddings\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[query_embedding[0].tolist()],\n",
    "                n_results=n_results\n",
    "            )\n",
    "            \n",
    "            retrieved_docs = []\n",
    "            \n",
    "            # Check if we have results - ChromaDB returns results as a list of lists (one per query)\n",
    "            if results.get('documents') and len(results['documents']) > 0:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results.get('metadatas', [[]])[0] if results.get('metadatas') else []\n",
    "                distances = results.get('distances', [[]])[0] if results.get('distances') else []\n",
    "                ids = results.get('ids', [[]])[0] if results.get('ids') else []\n",
    "                \n",
    "                if len(documents) > 0:\n",
    "                    for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                        similarity_score = 1 - distance\n",
    "                        if similarity_score >= score_threshold:\n",
    "                            retrieved_docs.append({\n",
    "                                'id': doc_id,\n",
    "                                'content': document,\n",
    "                                'metadata': metadata,\n",
    "                                'similarity_score': similarity_score,\n",
    "                                'distance': distance,\n",
    "                                'rank': i+1\n",
    "                            })\n",
    "            \n",
    "            return retrieved_docs\n",
    "        except Exception as ex:\n",
    "            raise ValueError(f\"Error querying collection: {ex}\")\n",
    "\n",
    "# Initialize RAG Retriever\n",
    "rag_retriever = RagRetriever(vector_store_manager, embedding_manager)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3888dd",
   "metadata": {},
   "source": [
    "## Setting Up Ollama Server\n",
    "\n",
    "Before using RAGChain, you need to start the Ollama server. Here's how:\n",
    "\n",
    "### Step 1: Install Ollama\n",
    "\n",
    "If you haven't installed Ollama yet:\n",
    "\n",
    "**macOS:**\n",
    "```bash\n",
    "# Download from https://ollama.com/download or use Homebrew\n",
    "brew install ollama\n",
    "```\n",
    "\n",
    "**Linux:**\n",
    "```bash\n",
    "# Download from https://ollama.com/download or use curl\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "**Windows:**\n",
    "- Download the installer from https://ollama.com/download\n",
    "\n",
    "### Step 2: Start Ollama Server\n",
    "\n",
    "**Option A: Start in Terminal (Recommended for testing)**\n",
    "```bash\n",
    "# Start the server in a terminal window\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "The server will run on `http://localhost:11434` by default.\n",
    "\n",
    "**Option B: Start as Background Service (macOS/Linux)**\n",
    "```bash\n",
    "# On macOS, Ollama typically runs as a service automatically after installation\n",
    "# Check if it's running:\n",
    "ollama list\n",
    "\n",
    "# If not running, start it:\n",
    "ollama serve &\n",
    "```\n",
    "\n",
    "**Option C: Check if Already Running**\n",
    "```bash\n",
    "# Test if Ollama is already running\n",
    "ollama list\n",
    "# If this works, the server is already running!\n",
    "```\n",
    "\n",
    "### How to Stop Ollama Server\n",
    "\n",
    "**Option 1: If running in a terminal (foreground)**\n",
    "```bash\n",
    "# Press Ctrl+C in the terminal where ollama serve is running\n",
    "```\n",
    "\n",
    "**Option 2: Find process by port (Recommended if you get \"address already in use\" error)**\n",
    "```bash\n",
    "# Find the process using port 11434 (Ollama's default port)\n",
    "# macOS/Linux:\n",
    "lsof -i :11434\n",
    "\n",
    "# Alternative on Linux:\n",
    "netstat -tulpn | grep 11434\n",
    "# or\n",
    "ss -tulpn | grep 11434\n",
    "\n",
    "# Kill the process by PID (replace <PID> with the actual process ID from above)\n",
    "kill <PID>\n",
    "\n",
    "# Or force kill if needed\n",
    "kill -9 <PID>\n",
    "```\n",
    "\n",
    "**Option 3: Find and kill the process by name**\n",
    "```bash\n",
    "# Find the Ollama process\n",
    "ps aux | grep ollama\n",
    "\n",
    "# Kill the process by PID (replace <PID> with the actual process ID)\n",
    "kill <PID>\n",
    "\n",
    "# Or force kill if needed\n",
    "kill -9 <PID>\n",
    "```\n",
    "\n",
    "**Option 4: Kill all Ollama processes**\n",
    "```bash\n",
    "# Kill all ollama processes\n",
    "pkill ollama\n",
    "\n",
    "# Or force kill all\n",
    "pkill -9 ollama\n",
    "```\n",
    "\n",
    "**Option 5: Using systemctl (Linux with systemd)**\n",
    "```bash\n",
    "# Stop the service\n",
    "sudo systemctl stop ollama\n",
    "\n",
    "# Disable auto-start on boot (optional)\n",
    "sudo systemctl disable ollama\n",
    "```\n",
    "\n",
    "**Option 6: Using launchctl (macOS)**\n",
    "```bash\n",
    "# Stop the service\n",
    "launchctl unload ~/Library/LaunchAgents/com.ollama.ollama.plist\n",
    "\n",
    "# Or if installed system-wide\n",
    "sudo launchctl unload /Library/LaunchDaemons/com.ollama.ollama.plist\n",
    "```\n",
    "\n",
    "**Verify it's stopped:**\n",
    "```bash\n",
    "# This should fail if Ollama is stopped\n",
    "ollama list\n",
    "```\n",
    "\n",
    "### Step 3: Download a Model\n",
    "\n",
    "You need to download at least one model before using RAGChain:\n",
    "\n",
    "```bash\n",
    "# Download one or more models (choose based on your needs):\n",
    "# Single model:\n",
    "ollama pull llama2\n",
    "\n",
    "# Or download multiple models at once:\n",
    "for model in llama2 llama3 mistral phi gemma:2b; do ollama pull $model; done\n",
    "\n",
    "# Popular models:\n",
    "# - llama2: Llama 2 (7B parameters)\n",
    "# - llama3: Llama 3 (8B parameters)  \n",
    "# - mistral: Mistral 7B\n",
    "# - phi: Phi-2 (smaller, faster)\n",
    "# - gemma:2b: Gemma 2B (very fast)\n",
    "\n",
    "# Check downloaded models:\n",
    "ollama list\n",
    "```\n",
    "\n",
    "### Step 4: Verify Setup\n",
    "\n",
    "Run the cell below to verify Ollama is working:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eeb55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ollama is running!\n",
      "\n",
      "Available models:\n",
      "  - Unknown (1.56 GB)\n",
      "  - Unknown (1.49 GB)\n",
      "  - Unknown (4.07 GB)\n",
      "  - Unknown (3.56 GB)\n",
      "  - Unknown (1.88 GB)\n",
      "  - Unknown (4.58 GB)\n",
      "  - Unknown (4.58 GB)\n",
      "  - Unknown (4.34 GB)\n",
      "\n",
      "✓ Testing Ollama connection with model: Unknown...\n",
      "  ⚠️  Could not test generation: model 'Unknown' not found (status code: 404)\n",
      "  But Ollama server is running. You can proceed.\n"
     ]
    }
   ],
   "source": [
    "# Verify Ollama is running and check available models\n",
    "try:\n",
    "    import ollama\n",
    "    \n",
    "    # List available models\n",
    "    models_response = ollama.list()\n",
    "    available_models = []\n",
    "    if models_response and 'models' in models_response:\n",
    "        for model in models_response['models']:\n",
    "            model_name = model.get('name', 'Unknown')\n",
    "            available_models.append(model_name)\n",
    "    \n",
    "    if available_models:\n",
    "        # Test a simple generation with the first available model\n",
    "        test_response = ollama.generate(\n",
    "            model=available_models[0],\n",
    "            prompt='Say \"Hello\" if you can read this.'\n",
    "        )\n",
    "    \n",
    "except ImportError:\n",
    "    pass\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f8d675",
   "metadata": {},
   "source": [
    "## RAG Chain with Ollama LLM\n",
    "\n",
    "The RAGChain class combines retrieval with LLM generation using Ollama. It retrieves relevant context documents and uses them to generate contextualized answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9acc35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGChain class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class RAGChain:\n",
    "    \"\"\"\n",
    "    A RAG (Retrieval-Augmented Generation) chain that combines document retrieval\n",
    "    with LLM-powered response generation using Ollama.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        rag_retriever,\n",
    "        model_name: str = \"llama2\",\n",
    "        num_context_docs: int = 5,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the RAG Chain.\n",
    "        \n",
    "        Args:\n",
    "            rag_retriever: RagRetriever instance for document retrieval\n",
    "            model_name: Name of the Ollama model to use (e.g., \"llama2\", \"mistral\", \"phi\")\n",
    "            num_context_docs: Number of retrieved documents to include in context\n",
    "            temperature: Temperature for generation (0.0 to 1.0, default: 0.7)\n",
    "            max_tokens: Maximum tokens for response (None for no limit)\n",
    "        \"\"\"\n",
    "        self.rag_retriever = rag_retriever\n",
    "        self.model_name = model_name\n",
    "        self.num_context_docs = num_context_docs\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "        # Verify Ollama is available\n",
    "        self._verify_ollama()\n",
    "    \n",
    "    def _verify_ollama(self):\n",
    "        \"\"\"Verify that Ollama is running and the model is available.\"\"\"\n",
    "        try:\n",
    "            # Check if Ollama is running by listing models\n",
    "            models = ollama.list()\n",
    "            available_models = [model['name'] for model in models.get('models', [])]\n",
    "            \n",
    "            # Check if our model is available (handle model name variations)\n",
    "            model_found = False\n",
    "            for model in available_models:\n",
    "                if self.model_name in model or model in self.model_name:\n",
    "                    model_found = True\n",
    "                    self.model_name = model  # Use the exact model name\n",
    "                    break\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    def _format_prompt(self, query: str, context_docs: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Format the prompt with retrieved context and user query.\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            context_docs: List of retrieved document dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            Formatted prompt string\n",
    "        \"\"\"\n",
    "        if not context_docs:\n",
    "            # No context retrieved - still answer but note the limitation\n",
    "            prompt = f\"\"\"You are a helpful assistant. Answer the following question based on your knowledge.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        else:\n",
    "            # Format context from retrieved documents\n",
    "            context_parts = []\n",
    "            for i, doc in enumerate(context_docs, 1):\n",
    "                content = doc.get('content', '')\n",
    "                metadata = doc.get('metadata', {})\n",
    "                source = metadata.get('source', 'Unknown')\n",
    "                page = metadata.get('page', 'N/A')\n",
    "                \n",
    "                context_parts.append(f\"[Context {i} - Source: {source}, Page: {page}]\\n{content}\")\n",
    "            \n",
    "            context_text = \"\\n\\n\".join(context_parts)\n",
    "            \n",
    "            prompt = f\"\"\"Use the following context documents to answer the question. If the answer cannot be found in the context, say so clearly.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the context above:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        query: str,\n",
    "        num_context_docs: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        max_tokens: Optional[int] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate a response using RAG (Retrieval-Augmented Generation).\n",
    "        \n",
    "        Args:\n",
    "            query: User query to answer\n",
    "            num_context_docs: Override default number of context documents\n",
    "            temperature: Override default temperature\n",
    "            max_tokens: Override default max tokens\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "                - 'response': Generated answer\n",
    "                - 'context_docs': Retrieved context documents\n",
    "                - 'model': Model used\n",
    "                - 'query': Original query\n",
    "        \"\"\"\n",
    "        # Use provided parameters or fall back to instance defaults\n",
    "        num_docs = num_context_docs if num_context_docs is not None else self.num_context_docs\n",
    "        temp = temperature if temperature is not None else self.temperature\n",
    "        max_toks = max_tokens if max_tokens is not None else self.max_tokens\n",
    "        \n",
    "        # Retrieve relevant context documents\n",
    "        context_docs = self.rag_retriever.query(query, n_results=num_docs)\n",
    "        \n",
    "        # Format prompt with context\n",
    "        prompt = self._format_prompt(query, context_docs)\n",
    "        \n",
    "        # Prepare generation options\n",
    "        options = {\n",
    "            'temperature': temp\n",
    "        }\n",
    "        if max_toks is not None:\n",
    "            options['num_predict'] = max_toks\n",
    "        \n",
    "        # Generate response using Ollama\n",
    "        try:\n",
    "            response = ollama.generate(\n",
    "                model=self.model_name,\n",
    "                prompt=prompt,\n",
    "                options=options\n",
    "            )\n",
    "            \n",
    "            generated_text = response.get('response', '')\n",
    "            \n",
    "            return {\n",
    "                'response': generated_text,\n",
    "                'context_docs': context_docs,\n",
    "                'model': self.model_name,\n",
    "                'query': query,\n",
    "                'num_context_docs': len(context_docs)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error generating response: {e}\"\n",
    "            return {\n",
    "                'response': f\"Error: {error_msg}\",\n",
    "                'context_docs': context_docs,\n",
    "                'model': self.model_name,\n",
    "                'query': query,\n",
    "                'num_context_docs': len(context_docs),\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8609b3",
   "metadata": {},
   "source": [
    "### Initialize RAGChain\n",
    "\n",
    "Create a RAGChain instance using the existing rag_retriever. You can specify:\n",
    "- Model name (e.g., \"llama2\", \"mistral\", \"phi\", \"llama3\")\n",
    "- Number of context documents to retrieve (default: 5)\n",
    "- Temperature for generation (default: 0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d423e28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not verify Ollama connection: 'name'\n",
      "Make sure Ollama is running. You can start it with: ollama serve\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAGChain with default settings\n",
    "# Make sure Ollama is running and you have a model installed (e.g., ollama pull llama2)\n",
    "rag_chain = RAGChain(\n",
    "    rag_retriever=rag_retriever,\n",
    "    model_name=\"llama2\",  # Change to your preferred model\n",
    "    num_context_docs=5,    # Number of documents to retrieve\n",
    "    temperature=0.7        # Generation temperature\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae8341",
   "metadata": {},
   "source": [
    "### Example Queries\n",
    "\n",
    "Test the RAGChain with sample queries. The system will:\n",
    "1. Retrieve relevant documents from the vector store\n",
    "2. Format them into a prompt with context\n",
    "3. Generate a contextualized answer using Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d05ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving top 5 relevant documents...\n",
      " Generating embeddings for 1 texts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with shape (1, 384)\n",
      "Debug - Results structure: documents=1, first query has 5 results\n",
      "Debug - Found 5 documents from query, 5 ids, 5 distances\n",
      "Retrieved 0 documents after filtering by similarity score 0.0\n",
      "Retrieved 0 context documents\n",
      "Generating response using model: llama2...\n",
      "================================================================================\n",
      "Query: What is the main topic of this document?\n",
      "Model: llama2\n",
      "Context Documents Used: 0\n",
      "\n",
      "================================================================================\n",
      "Generated Response:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Based on the content of the document, the main topic appears to be \"Understanding the Importance of Sustainable Development in Today's World\".\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple query\n",
    "query = \"What is the main topic of this document?\"\n",
    "result = rag_chain.generate(query)\n",
    "result['response']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133eb5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving top 10 relevant documents...\n",
      " Generating embeddings for 1 texts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with shape (1, 384)\n",
      "Debug - Results structure: documents=1, first query has 10 results\n",
      "Debug - Found 10 documents from query, 10 ids, 10 distances\n",
      "Retrieved 10 documents after filtering by similarity score 0.0\n",
      "Retrieved 10 context documents\n",
      "Generating response using model: llama2...\n",
      "================================================================================\n",
      "Query: Explain neural networks in simple terms\n",
      "\n",
      "Generated Response:\n",
      "--------------------------------------------------------------------------------\n",
      "Neural networks are a type of machine learning that are inspired by the structure and function of the human brain. They are made up of layers of interconnected nodes or \"neurons,\" which process and transmit information. Each neuron receives input from other neurons, performs a computation on that input, and then sends the output to other neurons in the next layer. This process continues until the network reaches an output layer, where the final output is produced.\n",
      "\n",
      "The key to neural networks is their ability to learn and adapt through a process called backpropagation. Backpropagation allows the network to adjust its connections and weights based on the input it receives and the desired output. This allows the network to improve its performance over time, making it useful for tasks such as image recognition, language translation, and speech recognition.\n",
      "\n",
      "In simple terms, a neural network is like a complex system of interconnected nodes that work together to process and transmit information. It can be trained to perform specific tasks, such as recognizing images or understanding natural language, by adjusting its connections and weights based on feedback from the environment.\n",
      "================================================================================\n",
      "\n",
      "Context Sources:\n",
      "1. Similarity: 0.4335, Page: 9\n",
      "2. Similarity: 0.3428, Page: 6\n",
      "3. Similarity: 0.3228, Page: 8\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Query with custom parameters\n",
    "query = \"Explain neural networks in simple terms\"\n",
    "result = rag_chain.generate(\n",
    "    query,\n",
    "    num_context_docs=10,  # Retrieve more context\n",
    "    temperature=0.5        # Lower temperature for more focused responses\n",
    ")\n",
    "\n",
    "result['response']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ae5bd3",
   "metadata": {},
   "source": [
    "### Customizing the Prompt Template\n",
    "\n",
    "You can modify the `_format_prompt` method in the RAGChain class to customize how context and queries are formatted. The default template includes:\n",
    "- Clear instructions for the LLM\n",
    "- Retrieved context documents with source information\n",
    "- The user's question\n",
    "- Instructions to answer based on context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8bb482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Different model or configuration\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# # Using a different model\n",
    "# rag_chain_mistral = RAGChain(\n",
    "#     rag_retriever=rag_retriever,\n",
    "#     model_name=\"mistral\",\n",
    "#     num_context_docs=8,\n",
    "#     temperature=0.8\n",
    "# )\n",
    "\n",
    "# # Query with the new model\n",
    "# result = rag_chain_mistral.generate(\"What are the key concepts discussed?\")\n",
    "# print(result['response'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e8dda6",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "**If you get connection errors:**\n",
    "- Make sure Ollama is running: `ollama serve`\n",
    "- Check available models: `ollama list`\n",
    "- Pull a model if needed: `ollama pull llama2`\n",
    "\n",
    "**If no context is retrieved:**\n",
    "- Check that documents are properly indexed in the vector store\n",
    "- Try adjusting the `score_threshold` in `rag_retriever.query()`\n",
    "- Verify embeddings were generated with the same model used for queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e06d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1527 documents to collection pdf_documents\n",
      "Collection size: 7635\n",
      " Generating embeddings for 1 texts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with shape (1, 384)\n",
      "Retrieved document 0 documents after filtering by similarity score 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_manager.add_documents(chunked_docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38326236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Collection Diagnostics ===\n",
      "Collection name: pdf_documents\n",
      "Collection count: 7635\n",
      "\n",
      "=== Testing Query ===\n",
      " Generating embeddings for 1 texts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with shape (1, 384)\n",
      "Retrieved document 5 documents after filtering by similarity score 0.0\n",
      "\n",
      "Query returned 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f1e28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Clearing and Re-indexing Collection ===\n",
      "Deleted collection: pdf_documents\n",
      "Initializing Chroma client for collection pdf_documents...\n",
      "Collection pdf_documents initialized successfully\n",
      "Existing documents in collection: 0\n",
      "\n",
      "=== Re-adding documents ===\n",
      "Added 1527 documents to collection pdf_documents\n",
      "Collection size: 1527\n",
      "\n",
      "=== Testing Query After Re-indexing ===\n",
      " Generating embeddings for 1 texts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with shape (1, 384)\n",
      "Retrieved document 0 documents after filtering by similarity score 0.0\n",
      "\n",
      "❌ Still no results. Check the debug output above.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d7aee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generating embeddings for 1 texts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with shape (1, 384)\n",
      "Retrieved document 0 documents after filtering by similarity score 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c72cc222",
   "metadata": {},
   "source": [
    "## Important: Re-indexing Documents\n",
    "\n",
    "If you're still seeing gibberish results, the vector store may contain documents embedded with a different model. You need to:\n",
    "1. Clear the existing vector store\n",
    "2. Re-add documents with embeddings generated by the same model used for queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00846ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Clear the existing collection and re-add documents\n",
    "# Uncomment the lines below to clear and re-index\n",
    "\n",
    "# Delete the existing collection\n",
    "# vector_store_manager.client.delete_collection(name=vector_store_manager.collection_name)\n",
    "# print(\"Collection deleted. Re-run the VectorStoreManager cell to create a new one.\")\n",
    "\n",
    "# Then re-add documents:\n",
    "# vector_store_manager = VectorStoreManager()  # Creates new collection\n",
    "# vector_store_manager.add_documents(chunked_docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171fd24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af43094c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bd45e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
